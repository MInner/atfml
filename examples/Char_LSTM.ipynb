{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport atfml\n",
    "%aimport atfml.bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt\n",
    "def build_leo_tolstoy_dataset(path='/home/usman/data/char_lstm/warpeace_input.txt', \n",
    "                              n_data = 10000, window=10, batch_size=100):\n",
    "    with open(path) as file:\n",
    "        string = (''.join([next(file) for x in range(n_data)]))[1:].replace('\\n', ' ')\n",
    "    charset = list(zip(*Counter(string).most_common(70)))[0]\n",
    "    ch_idx = lambda ch: charset.index(ch) + 1 if ch in charset else 0\n",
    "    X_list, y_list = [], []\n",
    "    for substr, target_char in [(string[i:i+window], string[i+window]) for i in range(n_data)]:\n",
    "        x_row = [ch_idx(char) for char in substr]\n",
    "        y = ch_idx(target_char)\n",
    "        X_list.append(x_row)\n",
    "        y_list.append(y)\n",
    "    n_batches = int(n_data/batch_size)\n",
    "    X = np.array(X_list)[:n_batches*batch_size]\n",
    "    y = np.array(y_list)[:n_batches*batch_size]\n",
    "    \n",
    "    return ([{'X': X_batch, 'y': y_batch} \n",
    "            for X_batch, y_batch \n",
    "            in zip(np.split(X, n_batches, axis=0), \n",
    "                   np.split(y, n_batches, axis=0))], \n",
    "            \n",
    "            {'charset_size': len(charset), \n",
    "             'mapping': dict(zip(charset, range(len(charset)))),\n",
    "             'back_mapping': dict(zip(range(len(charset)), charset))}\n",
    "           )\n",
    "\n",
    "def prediction_into_string(pred, data_spec, err = '#'):\n",
    "    if err in data_spec['mapping'].keys():\n",
    "        raise ValueError('Wow, change err, it should not be in the mapping')\n",
    "    \n",
    "    bm = data_spec['back_mapping']\n",
    "    return ''.join([(bm[n-1] if n > 0 else err) for n in pred.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation without unit bundles (explicit one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 2: Tesla K40c (CNMeM is disabled, cuDNN 4007)\n",
      "/home/usman/anaconda2/envs/autograd_py3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shapes are: {'E': (70, 100), 'b_f': (100,), 'b_c': (100,), 'b_o': (100,), 'V_c': (100, 100), 'W_f': (200, 100), 'W_i': (200, 100), 'W_c': (200, 100), 'b_i': (100,), 'W_o': (200, 100), 'h_0': (100,), 'c_0': (100,)}, n_total_params: 97600\n",
      "FAST_COMPILE\n",
      "Building learning step function and gradient .... done\n",
      "Building  loss.. done\n",
      "Building  score.. done\n",
      "Building  predict.. done\n",
      "Charset:  70\n",
      "0        499       0.19\n",
      ">> t I warn you, if you don't tel | ss8onatatatatatatatatatatatatatatatatatatatatatata\n",
      "50      278.8       0.23\n",
      ">>  or Haugwitz either. This famo | the hat has have haveres and the has the has the h\n",
      "100      225.5        0.4\n",
      ">> ell, Prince, so Genoa and Lucc | e said the was the was the was the was the was the\n",
      "150      245.6       0.31\n",
      ">>  And I don't believe a word th | e of the of the of the of the of the of the of the\n",
      "200      202.9       0.47\n",
      ">> enoa and Lucca are now just fa | the was the was the was the was the was the was th\n",
      "250      224.1       0.39\n",
      ">> werless before him.... And I d | od and I and I and I and I and I and I and I and I\n",
      "300      199.2       0.46\n",
      ">> apartes. But I warn you, if yo | u were the prince of the prince of the prince of t\n",
      "350      219.2       0.42\n",
      ">> ays, or Haugwitz either. This  | and the Englovna and the Englovna and the Englovna\n",
      "400      171.5       0.51\n",
      ">>  Lucca are now just family est |  be her father. It in math I conel mation of math \n",
      "450      190.1       0.44\n",
      ">> ve a word that Hardenburg says |  will his vocand the Englovna of the Englovna of t\n",
      "500      166.9       0.57\n",
      ">> mily estates of the Buonaparte |  man of mather father can old the were the were th\n",
      "550      183.3       0.53\n",
      ">> believe a word that Hardenburg | , and the England the England the England the Engl\n",
      "600      146.9        0.6\n",
      ">>  of the Buonapartes. But I war |  would be that the ban of mate that is Anna Pavlov\n",
      "650      162.5       0.56\n",
      ">> either. This famous Prussian n | ever one of the noble that and the Empression of t\n",
      "700      137.4       0.64\n",
      ">> rtes. But I warn you, if you d | on't betwe would be nothing more said the prince d\n",
      "750      145.8       0.63\n",
      ">>  a word that Hardenburg says,  | of our of our did not for conver conver conver con\n",
      "800      128.8       0.68\n",
      ">> st family estates of the Buona | parte by could you were considered of the word tha\n",
      "850      139.2       0.66\n",
      ">>  or Haugwitz either. This famo | ut of the good of the English have not understand \n",
      "900      117.4       0.71\n",
      ">> I warn you, if you don't tell  | me the would you calmpgly a maning unhor reply. He\n",
      "950      121.8       0.68\n",
      ">> z either. This famous Prussian |  which our good of the English has and the one tha\n",
      "1000      101.4       0.78\n",
      ">> f the Buonapartes. But I warn  | a man you were out every could your father nonly a\n",
      "1050      119.5       0.69\n",
      ">>  word that Hardenburg says, or |  and had become more the geny of the Empress all t\n",
      "1100      99.84       0.77\n",
      ">> n you, if you don't tell me th | at the woman of the wants Baron Funke him mank wou\n",
      "1150      109.8       0.71\n",
      ">> ve a word that Hardenburg says |  in a conver me of habit will not and cannot under\n",
      "1200      88.92       0.79\n",
      ">> st family estates of the Buona | parte that the Empressation and at the Empressatio\n",
      "1250        116       0.75\n",
      ">> word that Hardenburg says, or  | his one has become must all habur disome must a ma\n",
      "1300        111       0.77\n",
      ">> o Genoa and Lucca are now just |  family in a nothing u's looking unhone said Anna \n",
      "1350      103.7       0.79\n",
      ">> witz either. This famous Pruss | ia allation of our dary, though have days. That is\n",
      "1400      108.6       0.75\n",
      ">>  so Genoa and Lucca are now ju | st family even wished it al show Baron Funke beauc\n",
      "1450      104.3       0.75\n",
      ">> s, or Haugwitz either. This fa | cess the some more to commended to sood come more \n",
      "1500      99.15       0.77\n",
      ">> oa and Lucca are now just fami | ly they with have a sered at last, and more him,\" \n",
      "1550      90.99       0.81\n",
      ">> at Hardenburg says, or Haugwit | z a fasili alokjoven which has burnt wish. He will\n",
      "1600      103.6       0.76\n",
      ">> . But I warn you, if you don't |  tell me a chat has been recied.  \"What would you \n",
      "1650        104       0.75\n",
      ">> is powerless before him.... An | d I don't understand the good and will somed affor\n",
      "1700      96.65       0.78\n",
      ">> . But I warn you, if you don't |  speak the conough have though I don't speak that \n",
      "1750      118.9       0.71\n",
      ">>  I don't believe a word that H | er his viss the joy she sad commercial spirit will\n",
      "1800      89.73       0.74\n",
      ">> e, so Genoa and Lucca are now  | I cannot and that all an ey our frips I don't spea\n",
      "1850      87.95        0.8\n",
      ">> z either. This famous Prussian |  ner who was the fecretimes the perstand had be th\n",
      "1900      98.87       0.75\n",
      ">> But I warn you, if you don't t | he therer, know I have you were mentioned him,\" sh\n",
      "1950      96.78       0.79\n",
      ">> powerless before him.... And I |  don't believe some more teartion on be noble the \n",
      "2000      96.68       0.74\n",
      ">> rn you, if you don't tell me t | hat since. \"Lavathy is an as you? If you had and t\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff67458bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from atfml.core import AutoGradBackend, TheanoBackend\n",
    "import atfml.utils as utils\n",
    "from atfml.utils import inits, behaviours as bhvs\n",
    "\n",
    "# bk = AutoGradBackend()\n",
    "bk = TheanoBackend()\n",
    "\n",
    "class LSTMCharModel(bk.ModelLoss):\n",
    "    def __init__(self, *, vocab_size, n_hidden_dim, seq_len, data_spec):      \n",
    "        self.n_hidden_dim = n_hidden_dim\n",
    "        self.n_embedding_dim = n_hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.data_spec = data_spec\n",
    "        \n",
    "        w_width = self.n_embedding_dim + self.n_hidden_dim\n",
    "        \n",
    "        arg_dict = {\n",
    "            'default_init_method': inits.gaussian_init_with(mu=0, std=1),\n",
    "            'weight_template': {\n",
    "                'E': {'shape':(vocab_size, self.n_embedding_dim) },\n",
    "                \n",
    "                'h_0': {'shape':(n_hidden_dim, ) },\n",
    "                'c_0': {'shape':(n_hidden_dim, ) },\n",
    "                'W_i': {'shape': (w_width, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "                'b_i': {'shape': (n_hidden_dim, ) },\n",
    "                'W_c': {'shape': (w_width, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "                'b_c': {'shape': (n_hidden_dim, ) },\n",
    "                'W_f': {'shape': (w_width, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "                'b_f': {'shape': (n_hidden_dim, ) },\n",
    "                'W_o': {'shape': (w_width, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "                'b_o': {'shape': (n_hidden_dim, ) },\n",
    "                'V_c': {'shape': (n_hidden_dim, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "            },\n",
    "            'data_template': {\n",
    "                'X': {'shape':('batch_size', self.seq_len), 'dtype': 'int64' },\n",
    "                'y': {'shape':('batch_size', ) , 'dtype': 'int64'},\n",
    "            },\n",
    "            'optimization_method': {'name': 'adam', 'learning_rate': 0.005, 'clip': 100},\n",
    "            'behaviours': {\n",
    "                'loss': bhvs.LossLogBehaviour(),\n",
    "            }\n",
    "        }\n",
    "        super().__init__(**arg_dict)\n",
    "    \n",
    "    @bk.export_data(['X'])\n",
    "    def predict(self, theta, data, const):\n",
    "        \"\"\"\n",
    "        i_t:[batch_size, hidden_dim]\n",
    "            = sigmoid( <h_{t-1}, x_t>:[batch_size, hid_dim + input_dim ] @ W_i:[hid+input, hid] + b_i[1, hid])\n",
    "        c_tilde_t:[batch_size, hidden_dim]\n",
    "            = tanh( <h_{t-1}, x_t>:[batch_size, hid_dim + input_dim ] @ W_c:[hid+input, hid] + b_c[1, hid])\n",
    "        f_t:[batch_size, hidden_dim]\n",
    "            = sigmoid( <h_{t-1}, x_t>:[batch_size, hid_dim + input_dim ] @ W_f:[hid+input, hid] + b_f[1, hid])\n",
    "        c_t:[batch_size, hidden_dim]\n",
    "            = i_t*c_tilde_t + f_t*c_{t-1}\n",
    "        o_t:[batch_size, hidden_dim]\n",
    "            = sigmoid( <h_{t-1}, x_t>:[batch_size, hid_dim + input_dim ] @ W_o:[hid+input, hid] \n",
    "                        + c_t @ V_c:[hid, hid]\n",
    "                        + b_o[1, hid])\n",
    "        h_t:[batch_size, hidden_dim]\n",
    "            = o_t*tanh(c_t)\n",
    "        \n",
    "        \"\"\"\n",
    "        np = None # for safety\n",
    "        h_prev = bk.repeat(theta.h_0[bk.newaxis, :], const.batch_size, axis=0)\n",
    "        c_prev = bk.repeat(theta.c_0[bk.newaxis, :], const.batch_size, axis=0)\n",
    "        \n",
    "        sigmoid = lambda x: x/(1+bk.abs(x+1e-20))\n",
    "        for i in range(self.seq_len):\n",
    "            x_t = data.X[:, i]\n",
    "            embedded_x_t = theta.E[x_t] # [batch_size, n_hidden_him]\n",
    "            concat_h_x = bk.concatenate([h_prev, embedded_x_t], axis=1)\n",
    "            i_t = sigmoid(bk.dot(concat_h_x, theta.W_i) + theta.b_i[bk.newaxis, :])\n",
    "            c_tld_t = sigmoid(bk.dot(concat_h_x, theta.W_c) + theta.b_c[bk.newaxis, :])\n",
    "            f_t = sigmoid(bk.dot(concat_h_x, theta.W_f) + theta.b_f[bk.newaxis, :])\n",
    "            c_t = bk.multiply(i_t, c_tld_t) + bk.multiply(f_t, c_prev)\n",
    "            o_t = sigmoid(bk.dot(concat_h_x, theta.W_o) + bk.dot(c_t, theta.V_c) \n",
    "                          + theta.b_o[bk.newaxis, :])\n",
    "            h_t = bk.multiply(o_t, bk.tanh(c_t))\n",
    "            h_prev = h_t\n",
    "            c_prev = c_t\n",
    "        \n",
    "        ## [batch_size, n_hidden_dim] * [n_hidden, vocab_size]\n",
    "        output_prob = bk.ops.softmax(bk.dot(h_prev, theta.E.T) )\n",
    "        \n",
    "        bk.assert_arr_shape({output_prob.shape: (const.batch_size, self.vocab_size)})\n",
    "        return output_prob\n",
    "    \n",
    "    def loss(self, theta, data, const):\n",
    "        np = None # for safety\n",
    "        output_prob = self.raw.predict(theta, data, const)\n",
    "        cross_ent_loss_i = -bk.log( output_prob[bk.arange(const.batch_size), data.y]  )\n",
    "        loss = bk.sum(cross_ent_loss_i)\n",
    "        return loss\n",
    "    \n",
    "    @bk.export\n",
    "    def score(self, theta, data, const):\n",
    "        np = None # for safety\n",
    "        output_prob = self.raw.predict(theta, data, const)\n",
    "        pred_class = bk.argmax(output_prob, axis=1)\n",
    "        score = bk.sum( bk.isclose(pred_class, data.y), dtype='float32')/const.batch_size\n",
    "        return score\n",
    "    \n",
    "    def step_callback(self, loss_val, theta, data, const, info):\n",
    "        bk = None\n",
    "        if info['n_iter'] % 50 == 0:\n",
    "            score_val = self.compiled.score(theta, data, const)\n",
    "            print('%d %10.4g %10.4g' % (info['n_iter'], loss_val, score_val))\n",
    "            self.generate_output(theta, data, const, info)\n",
    "            \n",
    "    def generate_output(self, theta, data, const, info):\n",
    "        ## print random prediction as a string\n",
    "        i_to_check = np.random.randint(const.batch_size)\n",
    "        data_line = data.X[i_to_check].reshape((1, -1))\n",
    "        data_line_y = data.y[i_to_check].reshape((-1, ))\n",
    "        original_string = prediction_into_string(data_line.ravel(), self.data_spec)\n",
    "\n",
    "        long_pred_row = []\n",
    "        for i in range(50):\n",
    "            small_data = utils.to_record({'X': data_line}) # we don't care about y\n",
    "            local_const = utils.to_record({'batch_size': 1}) # dirty stub :(\n",
    "            output_prob = self.compiled.predict(theta, small_data, local_const)\n",
    "            pred = np.argmax(output_prob, axis=1).ravel()\n",
    "            new_dataline = data_line[:, 1:].ravel().tolist()+pred.tolist()\n",
    "            data_line = np.array(new_dataline).reshape((1, -1))\n",
    "            long_pred_row.append(pred[0])\n",
    "\n",
    "        string = prediction_into_string(np.array(long_pred_row), self.data_spec)\n",
    "        print('>>', original_string, '|', string)\n",
    "            \n",
    "def test_lstm_on_leo():\n",
    "    data, data_spec = build_leo_tolstoy_dataset(n_data = 10000, window=30, batch_size=100)\n",
    "    model = LSTMCharModel(n_hidden_dim=100, vocab_size=data_spec['charset_size'], \n",
    "                          seq_len=30, data_spec=data_spec)\n",
    "    print('Charset: ', data_spec['charset_size'])\n",
    "    best_theta = model.fit(data, n_max_steps=2000)\n",
    "    plt.plot(model.behaviours.loss.log)\n",
    "    plt.show()\n",
    "    \n",
    "test_lstm_on_leo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with unit bundles (LSTM logic is hidden):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../bundles/lstm_encoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../bundles/lstm_encoder.py\n",
    "\n",
    "# defined in atfml/examples/Char_LSTM.ipynb\n",
    "from atfml.core import UnitBundle\n",
    "from atfml.utils import inits\n",
    "\n",
    "class LSTMEncoder(UnitBundle):\n",
    "    def __init__(self, bk, name, *, n_hidden_dim, n_input_dim, n_steps, masking=False):\n",
    "        self.n_steps = n_steps\n",
    "        w_width = n_input_dim + n_hidden_dim - int(masking) # if m then -1 dim\n",
    "        self.masking = masking\n",
    "        \n",
    "        arg_dict = {\n",
    "            'name': name,\n",
    "            'weight_template': {\n",
    "                'h_0': {'shape': (n_hidden_dim, ) },\n",
    "                'c_0': {'shape': (n_hidden_dim, ) },\n",
    "                'W_i': {'shape': (w_width, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "                'b_i': {'shape': (n_hidden_dim, ) },\n",
    "                'W_c': {'shape': (w_width, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "                'b_c': {'shape': (n_hidden_dim, ) },\n",
    "                'W_f': {'shape': (w_width, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "                'b_f': {'shape': (n_hidden_dim, ) },\n",
    "                'W_o': {'shape': (w_width, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "                'b_o': {'shape': (n_hidden_dim, ) },\n",
    "                'V_c': {'shape': (n_hidden_dim, n_hidden_dim), \n",
    "                        'init_method': inits.identity_repeat_init },\n",
    "            },\n",
    "            'data_template': {\n",
    "                'X': ('batch_size', 'seq_len', n_input_dim),\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        super().__init__(bk, **arg_dict)\n",
    "        \n",
    "    def _func(self, theta, data, const):\n",
    "        \"\"\"\n",
    "            i_t:[batch_size, hidden_dim]\n",
    "                = sigmoid( <h_{t-1}, x_t>:[batch_size, hid_dim + input_dim ] \n",
    "                        @ W_i:[hid+input, hid] + b_i[1, hid])\n",
    "            c_tilde_t:[batch_size, hidden_dim]\n",
    "                = tanh( <h_{t-1}, x_t>:[batch_size, hid_dim + input_dim ] \n",
    "                        @ W_c:[hid+input, hid] + b_c[1, hid])\n",
    "            f_t:[batch_size, hidden_dim]\n",
    "                = sigmoid( <h_{t-1}, x_t>:[batch_size, hid_dim + input_dim ] \n",
    "                            @ W_f:[hid+input, hid] + b_f[1, hid])\n",
    "            c_t:[batch_size, hidden_dim]\n",
    "                = i_t*c_tilde_t + f_t*c_{t-1}\n",
    "            o_t:[batch_size, hidden_dim]\n",
    "                = sigmoid( <h_{t-1}, x_t>:[batch_size, hid_dim + input_dim ] \n",
    "                            @ W_o:[hid+input, hid] \n",
    "                            + c_t @ V_c:[hid, hid] + b_o[1, hid])\n",
    "            h_t:[batch_size, hidden_dim]\n",
    "                = o_t*tanh(c_t)\n",
    "        \"\"\"\n",
    "        bk = self.bk\n",
    "        h_prev = self.bk.repeat(theta.h_0[bk.newaxis, :], const.batch_size, axis=0)\n",
    "        c_prev = self.bk.repeat(theta.c_0[bk.newaxis, :], const.batch_size, axis=0)\n",
    "        \n",
    "        sigmoid = lambda x: x/(1+bk.abs(x+1e-20))\n",
    "        for t in range(self.n_steps):\n",
    "            x_t = data.X[:, t]\n",
    "            if self.masking:\n",
    "                x_t = data.X[:, t, 1:]\n",
    "                mask_t = data.X[:, t, 1]\n",
    "                not_mask_t = (data.X[:, t, 1] + 1) % 2\n",
    "            \n",
    "            concat_h_x = bk.concatenate([h_prev, x_t], axis=1)\n",
    "            i_t = sigmoid(bk.dot(concat_h_x, theta.W_i) + theta.b_i[bk.newaxis, :])\n",
    "            c_tld_t = sigmoid(bk.dot(concat_h_x, theta.W_c) + theta.b_c[bk.newaxis, :])\n",
    "            f_t = sigmoid(bk.dot(concat_h_x, theta.W_f) + theta.b_f[bk.newaxis, :])\n",
    "            c_t = bk.multiply(i_t, c_tld_t) + bk.multiply(f_t, c_prev)\n",
    "            o_t = sigmoid(bk.dot(concat_h_x, theta.W_o) + bk.dot(c_t, theta.V_c) \n",
    "                          + theta.b_o[bk.newaxis, :])\n",
    "            h_t = bk.multiply(o_t, bk.tanh(c_t))\n",
    "            \n",
    "            if self.masking:\n",
    "                ## in theano sould be T.switch() - way more efficient\n",
    "                h_t_masked = (bk.multiply(h_t, mask_t[:, bk.newaxis]) \n",
    "                              + bk.multiply(h_prev, not_mask_t[:, bk.newaxis]))\n",
    "                h_prev = h_t_masked\n",
    "            else:\n",
    "                h_prev = h_t\n",
    "            \n",
    "            c_prev = c_t\n",
    "            \n",
    "        return h_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shapes are: {'encoder__W_f': (200, 100), 'encoder__W_o': (200, 100), 'encoder__b_c': (100,), 'encoder__b_f': (100,), 'encoder__c_0': (100,), 'encoder__h_0': (100,), 'encoder__b_o': (100,), 'encoder__b_i': (100,), 'encoder__W_c': (200, 100), 'E': (70, 100), 'encoder__W_i': (200, 100), 'encoder__V_c': (100, 100)}, n_total_params: 97600\n",
      "FAST_COMPILE\n",
      "Building learning step function and gradient .."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from atfml.core import AutoGradBackend, TheanoBackend\n",
    "import atfml.utils as utils\n",
    "from atfml.utils import inits, behaviours as bhvs\n",
    "from atfml.bundles.lstm_encoder import LSTMEncoder\n",
    "\n",
    "# bk = AutoGradBackend()\n",
    "bk = TheanoBackend()\n",
    "\n",
    "class LSTMCharModelWithBundles(bk.ModelLoss):\n",
    "    def __init__(self, *, vocab_size, n_hidden_dim, seq_len, data_spec):      \n",
    "        self.n_hidden_dim = n_hidden_dim\n",
    "        self.n_embedding_dim = n_hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.data_spec = data_spec\n",
    "        \n",
    "        arg_dict = {\n",
    "            'data_template': {\n",
    "                'X': {'shape':('batch_size', self.seq_len), 'dtype': 'int64' },\n",
    "                'y': {'shape':('batch_size', ) , 'dtype': 'int64'},\n",
    "            },\n",
    "            'default_init_method': inits.gaussian_init_with(mu=0, std=1),\n",
    "            'weight_template': {\n",
    "                'E': {'shape':(self.vocab_size, self.n_embedding_dim) },\n",
    "            },\n",
    "            'weight_bundles': {\n",
    "                'encoder': {'class': LSTMEncoder, \n",
    "                            'args': {'n_hidden_dim':n_hidden_dim, \n",
    "                                     'n_input_dim': self.n_embedding_dim, \n",
    "                                     'n_steps': seq_len} },\n",
    "                \n",
    "            },\n",
    "            'optimization_method': {'name': 'adam', 'learning_rate': 0.005, 'clip': 100},\n",
    "            'behaviours': {\n",
    "                'loss': bhvs.LossLogBehaviour(),\n",
    "            }\n",
    "        }\n",
    "        super().__init__(**arg_dict)\n",
    "    \n",
    "    @bk.export_data(['X'])\n",
    "    def predict(self, theta, data, const):\n",
    "        np = None # for safety\n",
    "        embedded_X = theta.E[data.X] # [batch_size, seq_len, n_hidden_him]\n",
    "        h_last = self.bundles.encoder.apply(theta, {'X':embedded_X})\n",
    "        ## [batch_size, n_hidden_dim] * [n_hidden, vocab_size]\n",
    "        output_prob = bk.ops.softmax(bk.dot(h_last, theta.E.T) )\n",
    "        bk.assert_arr_shape({output_prob.shape: (const.batch_size, self.vocab_size)})\n",
    "        return output_prob\n",
    "    \n",
    "    def loss(self, theta, data, const):\n",
    "        np = None # for safety\n",
    "        output_prob = self.raw.predict(theta, data, const)\n",
    "        cross_ent_loss_i = -bk.log( output_prob[bk.arange(const.batch_size), data.y]  )\n",
    "        loss = bk.sum(cross_ent_loss_i)\n",
    "        return loss\n",
    "    \n",
    "    @bk.export\n",
    "    def score(self, theta, data, const):\n",
    "        np = None # for safety\n",
    "        output_prob = self.raw.predict(theta, data, const)\n",
    "        pred_class = bk.argmax(output_prob, axis=1)\n",
    "        score = bk.sum( bk.isclose(pred_class, data.y), dtype='float32')/const.batch_size\n",
    "        return score\n",
    "    \n",
    "    def step_callback(self, loss_val, theta, data, const, info):\n",
    "        bk = None\n",
    "        if info['n_iter'] % 200 == 0:\n",
    "            score_val = self.compiled.score(theta, data, const)\n",
    "            print('%d %10.4g %10.4g' % (info['n_iter'], loss_val, score_val))\n",
    "            self.generate_output(theta, data, const, info)\n",
    "            \n",
    "    def generate_output(self, theta, data, const, info):\n",
    "        ## print random prediction as a string\n",
    "        i_to_check = np.random.randint(const.batch_size)\n",
    "        data_line = data.X[i_to_check].reshape((1, -1))\n",
    "        data_line_y = data.y[i_to_check].reshape((-1, ))\n",
    "        original_string = prediction_into_string(data_line.ravel(), self.data_spec)\n",
    "\n",
    "        long_pred_row = []\n",
    "        for i in range(100):\n",
    "            small_data = utils.to_record({'X': data_line}) # we don't care about y\n",
    "            local_const = utils.to_record({'batch_size': 1}) # dirty stub :(\n",
    "            output_prob = self.compiled.predict(theta, small_data, local_const)\n",
    "            pred = np.argmax(output_prob, axis=1).ravel()\n",
    "            new_dataline = data_line[:, 1:].ravel().tolist()+pred.tolist()\n",
    "            data_line = np.array(new_dataline).reshape((1, -1))\n",
    "            long_pred_row.append(pred[0])\n",
    "\n",
    "        string = prediction_into_string(np.array(long_pred_row), self.data_spec)\n",
    "        print('>>', original_string, '|', string)\n",
    "\n",
    "def test_lstm_on_leo():\n",
    "    data, data_spec = build_leo_tolstoy_dataset(n_data = 10000, window=30, batch_size=100)\n",
    "    model = LSTMCharModelWithBundles(n_hidden_dim=100, vocab_size=data_spec['charset_size'], \n",
    "                                      seq_len=30, data_spec=data_spec)\n",
    "    print('Charset: ', data_spec['charset_size'])\n",
    "    best_theta = model.fit(data, n_max_steps=2000)\n",
    "    plt.plot(model.behaviours.loss.log)\n",
    "    plt.show()\n",
    "    \n",
    "test_lstm_on_leo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano Compilation time as a function of n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 50 8712.224641\n",
      "10 100 8767.930523\n",
      "10 200 8834.854101\n",
      "10 400 8890.550324\n",
      "30 50 9132.092808\n",
      "30 100 9385.902774\n",
      "30 200 9615.920387\n",
      "30 400 9860.692185\n",
      "50 50 10382.272901\n",
      "50 100 10911.114515\n",
      "50 200 11431.13315\n",
      "50 400 11987.925409\n",
      "100 50 13726.152938\n",
      "100 100 15472.452608\n",
      "100 200 17237.639376\n",
      "100 400 19011.397555\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for seq_len in [10, 30, 50, 100]:\n",
    "    for n_hidden_dim in [50, 100, 200, 400]:\n",
    "        class LSTMCharModelWithBundles(bk.ModelLoss):\n",
    "            def __init__(self, *, vocab_size, n_hidden_dim, seq_len, data_spec):      \n",
    "                self.n_hidden_dim = n_hidden_dim\n",
    "                self.n_embedding_dim = n_hidden_dim\n",
    "                self.vocab_size = vocab_size\n",
    "                self.seq_len = seq_len\n",
    "                self.data_spec = data_spec\n",
    "\n",
    "                arg_dict = {\n",
    "                    'data_template': {\n",
    "                        'X': {'shape':('batch_size', self.seq_len), 'dtype': 'int64' },\n",
    "                        'y': {'shape':('batch_size', ) , 'dtype': 'int64'},\n",
    "                    },\n",
    "                    'default_init_method': inits.gaussian_init_with(mu=0, std=1),\n",
    "                    'weight_template': {\n",
    "                        'E': {'shape':(self.vocab_size, self.n_embedding_dim) },\n",
    "                    },\n",
    "                    'weight_bundles': {\n",
    "                        'encoder': {'class': LSTMEncoder, \n",
    "                                    'args': {'n_hidden_dim':n_hidden_dim, \n",
    "                                             'n_input_dim': self.n_embedding_dim, \n",
    "                                             'n_steps': seq_len} },\n",
    "\n",
    "                    },\n",
    "                    'optimization_method': {'name': 'adam', \n",
    "                                            'learning_rate': 0.005, \n",
    "                                            'clip': 100},\n",
    "                    'behaviours': {\n",
    "                        'loss': bhvs.LossLogBehaviour(),\n",
    "                    },\n",
    "                    'verbose': False,\n",
    "                }\n",
    "                super().__init__(**arg_dict)\n",
    "\n",
    "            @bk.export_data(['X'])\n",
    "            def predict(self, theta, data, const):\n",
    "                np = None # for safety\n",
    "                embedded_X = theta.E[data.X] # [batch_size, seq_len, n_hidden_him]\n",
    "                h_last = self.bundles.encoder.apply(theta, {'X':embedded_X})\n",
    "                ## [batch_size, n_hidden_dim] * [n_hidden, vocab_size]\n",
    "                output_prob = bk.ops.softmax(bk.dot(h_last, theta.E.T) )\n",
    "                bk.assert_arr_shape({output_prob.shape: \n",
    "                                     (const.batch_size, self.vocab_size)})\n",
    "                return output_prob\n",
    "\n",
    "            def loss(self, theta, data, const):\n",
    "                np = None # for safety\n",
    "                output_prob = self.raw.predict(theta, data, const)\n",
    "                idx = bk.arange(const.batch_size)\n",
    "                cross_ent_loss_i = -bk.log( output_prob[idx, data.y]  )\n",
    "                loss = bk.sum(cross_ent_loss_i)\n",
    "                return loss\n",
    "\n",
    "            def step_callback(self, loss_val, theta, data, const, info):\n",
    "                pass\n",
    "\n",
    "        now = time.clock()\n",
    "        model = LSTMCharModelWithBundles(n_hidden_dim=n_hidden_dim, \n",
    "                                         vocab_size=70,\n",
    "                                         seq_len=seq_len, data_spec={})\n",
    "        t_diff = time.clock()\n",
    "        print(seq_len, n_hidden_dim, t_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
