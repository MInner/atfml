{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport atfml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from atfml.utils import inits\n",
    "\n",
    "def build_rnn_final_dataset(n_hidden_dim=10, n_input_dim=15, n_batch_size=20,\n",
    "                            n_steps_per_batch=100, n_batches=30, data_noise=0.0, param_noise=0.05):\n",
    "    \"\"\"\n",
    "    dynamic system:\n",
    "        x_t - input\n",
    "        h_t = (h_{t-1} @ w_hh + x_t @ w_hi)\n",
    "        y_t = h_t\n",
    "    \n",
    "    data shape:\n",
    "        [.. ,(X_i, y_i), ..] - n_batches\n",
    "\n",
    "            X_i : (n_batch_size, n_steps, n_input_dim)\n",
    "            y_i : (n_batch_size, n_hidden_dim)\n",
    "    \"\"\"\n",
    "    n_steps = n_batch_size*n_steps_per_batch*n_batches\n",
    "    h_0 = np.random.randn(n_hidden_dim)\n",
    "    h = np.zeros((n_steps, n_hidden_dim))\n",
    "    x = np.random.randn(n_steps, n_input_dim)\n",
    "    w_hi = np.random.randn(n_input_dim, n_hidden_dim)\n",
    "    w_hh = (inits.identity_repeat_init(n_hidden_dim, n_hidden_dim) +\n",
    "            (np.random.rand(n_hidden_dim, n_hidden_dim)-0.5)*param_noise )\n",
    "    h_prev = h_0\n",
    "    activation = lambda x: np.maximum(0, 1-x)\n",
    "    for i in range(n_steps):\n",
    "        h[i] = activation(h_prev.dot(w_hh) + x[i].dot(w_hi))\n",
    "        h_prev = h[i]\n",
    "\n",
    "    x = x + np.random.random(x.shape)*data_noise\n",
    "    ## x of shape (n_batches*n_batch_size*n_steps_per_batch, n_input_dim)\n",
    "    ## h of shape (n_batches*n_batch_size*n_steps_per_batch, n_hidden_dim)\n",
    "    \n",
    "    data = []\n",
    "    for batch_n in range(n_batches-1):\n",
    "        batch_tensor_X_rows = []\n",
    "        batch_tensor_y_rows = []\n",
    "        for line_n in range(n_batch_size):\n",
    "            from_idx = batch_n*n_batch_size*n_steps_per_batch + line_n*n_steps_per_batch\n",
    "            to_idx =   batch_n*n_batch_size*n_steps_per_batch + (line_n+1)*n_steps_per_batch\n",
    "            batch_x = x[from_idx:to_idx]\n",
    "            batch_y = h[to_idx-1]\n",
    "            batch_tensor_X_rows.append(batch_x)\n",
    "            batch_tensor_y_rows.append(batch_y)\n",
    "        \n",
    "        data.append( {'X': np.array(batch_tensor_X_rows), 'y': np.array(batch_tensor_y_rows)} )\n",
    "        \n",
    "    return (h_0, w_hi, w_hh), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shapes are: {'h_0': (10,), 'W_hidden_to_hidden': (10, 10), 'W_input_to_hidden': (15, 10)}, n_total_params: 260\n",
      "FAST_COMPILE\n",
      "Building learning step function and gradient .."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from atfml.core import AutoGradBackend, TheanoBackend\n",
    "from atfml.utils import inits, behaviours\n",
    "\n",
    "# bk = AutoGradBackend()\n",
    "bk = TheanoBackend()\n",
    "\n",
    "class BasicRNN(bk.ModelLoss):\n",
    "    def __init__(self, *, n_hidden_dim=10, seq_steps=10, n_input_dim=10):\n",
    "        self.n_hidden_dim = n_hidden_dim\n",
    "        self.n_input_dim = n_input_dim\n",
    "        self.seq_steps = seq_steps\n",
    "        \n",
    "        arg_dict = {\n",
    "            'default_init_method': inits.gaussian_init_with(mu=0, std=1),\n",
    "            'weight_template': {\n",
    "                'h_0': {'shape':(self.n_hidden_dim, ) },\n",
    "                'W_input_to_hidden': {'shape': (n_input_dim, n_hidden_dim), \n",
    "                                      'init_method': inits.identity_repeat_init },\n",
    "                'W_hidden_to_hidden': {'shape':(n_hidden_dim, n_hidden_dim), \n",
    "                                       'init_method': inits.identity_repeat_init}\n",
    "            },\n",
    "            'data_template': {\n",
    "                'X': {'shape':('batch_size', seq_steps, n_input_dim), 'dtype': 'float64' },\n",
    "                'y': {'shape':('batch_size', n_hidden_dim) , 'dtype': 'float64'},\n",
    "            },\n",
    "            'optimization_method': {'name': 'adam', 'learning_rate': 0.01, 'clip': 100},\n",
    "            'behaviours': {\n",
    "                'loss': behaviours.LossLogBehaviour(),\n",
    "                'w_ih': behaviours.WieghtLogBehaviour('W_input_to_hidden'),\n",
    "                'w_hh': behaviours.WieghtLogBehaviour('W_hidden_to_hidden'),\n",
    "            }\n",
    "        }\n",
    "        super().__init__(**arg_dict)\n",
    "    \n",
    "    def predict(self, theta, data, const):\n",
    "        h_prev = np.repeat(theta.h_0[np.newaxis, :], const.batch_size, axis=0)\n",
    "        activation = lambda x: bk.maximum(0, 1-x)\n",
    "        to_stack = []\n",
    "        for i in range(self.seq_steps):\n",
    "            x_t = data.X[:, i, :]\n",
    "            input2hidden = bk.dot(x_t, theta.W_input_to_hidden)\n",
    "            hidden2hidden = bk.dot(h_prev, theta.W_hidden_to_hidden)\n",
    "            h_prev = activation(input2hidden + hidden2hidden)\n",
    "            to_stack.append(h_prev)\n",
    "        # no stack in autograd\n",
    "        output = bk.concatenate([x[:, bk.newaxis, :] for x in to_stack], axis=1)\n",
    "        bk.assert_arr_shape({output.shape: (const.batch_size, self.seq_steps, self.n_hidden_dim)})\n",
    "        return output\n",
    "    \n",
    "    def loss(self, theta, data, const):\n",
    "        pred = self.predict(theta, data, const)\n",
    "        last_step_pred = pred[:, -1, :]\n",
    "        loss = bk.sum((last_step_pred - data.y)**2)\n",
    "        return loss\n",
    "    \n",
    "    def step_callback(self, loss_val, theta, data, const, info):\n",
    "        if info['n_iter'] % 100 == 0:\n",
    "            print('%5d %10.4g' % (info['n_iter'], loss_val))\n",
    "            \n",
    "def test_rnn():\n",
    "    (h_0, w_ih, w_hh), data = build_rnn_final_dataset(n_hidden_dim=10, n_input_dim=15, \n",
    "                                                      n_batch_size=5, n_steps_per_batch=100, \n",
    "                                                      n_batches=30, data_noise=0.3, param_noise=0.2)\n",
    "    \n",
    "    model = BasicRNN(n_hidden_dim=10, n_input_dim=15, seq_steps=100)\n",
    "    best_theta = model.fit(data, n_max_steps=2000)\n",
    "    \n",
    "    plt.semilogy(range(len(model.behaviours.loss.log)), model.behaviours.loss.log)\n",
    "    plt.show()\n",
    "    \n",
    "    w_ih_dists = [np.sum((w_ih_learned_i-w_ih)**2) for w_ih_learned_i in model.behaviours.w_ih.log]\n",
    "    plt.semilogy(range(len(w_ih_dists)), w_ih_dists)\n",
    "    plt.show()\n",
    "    \n",
    "    w_hh_dists = [np.sum((w_hh_learned_i-w_hh)**2) for w_hh_learned_i in model.behaviours.w_hh.log]\n",
    "    plt.semilogy(range(len(w_hh_dists)), w_hh_dists)\n",
    "    plt.show()\n",
    "    \n",
    "test_rnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
