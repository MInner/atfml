{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile core.py\n",
    "\n",
    "import abc\n",
    "import collections\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "\n",
    "from atfml.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a core.py\n",
    "\n",
    "class AbstractModelLoss(object):    \n",
    "    def __init__(self, *, data_template, weight_template, default_init_method, \n",
    "                 optimization_method, default_type='float64', allow_downcast=False, \n",
    "                 test_forward_run=True, verbose=True, behaviours = {}, weight_bundles=None):\n",
    "        \n",
    "        if allow_downcast:\n",
    "            raise NotImplementedError(\"allow_downcast does not work properly with theano\")\n",
    "        \n",
    "        self._process_wieght_bundles(weight_bundles, weight_template)\n",
    "        \n",
    "        self.default_dtype = 'float64'\n",
    "        self.args = SimpleNamespace()\n",
    "        self.args.__dict__.update({\n",
    "            'test_forward_run': test_forward_run,\n",
    "            'verbose': verbose,\n",
    "            'default_init_method': default_init_method,\n",
    "            'weight_template': weight_template,\n",
    "            'data_template': data_template,\n",
    "            'optimization_method': optimization_method,\n",
    "            'allow_downcast': allow_downcast,\n",
    "            'behaviours': behaviours\n",
    "        })\n",
    "        \n",
    "        self._parse_data_shapes()\n",
    "        self._parse_weight_shapes()\n",
    "        self._parse_optimization_method()\n",
    "        \n",
    "        self._collect_exported_functions()\n",
    "        \n",
    "    def _process_wieght_bundles(self, weight_bundles, weight_template):\n",
    "        self.bundles = SimpleNamespace()\n",
    "        if weight_bundles != None:\n",
    "            for bundle_name, bundle_dict in weight_bundles.items():\n",
    "                bundle_class = bundle_dict['class']\n",
    "                bundle_obj = bundle_class(bk=self.bk, name=bundle_name, **bundle_dict['args'])\n",
    "                setattr(self.bundles, bundle_name, bundle_obj)\n",
    "                weight_template.update(bundle_obj.weight_template)\n",
    "                bundle_obj.registered = True\n",
    "        \n",
    "    def _parse_data_shapes(self):\n",
    "        req_names, spec_dicts = zip(*self.args.data_template.items())\n",
    "        req_shapes = [spec_dict['shape'] for spec_dict in spec_dicts]\n",
    "        req_dtypes = [spec_dict.get('dtype', self.default_dtype) for spec_dict in spec_dicts]\n",
    "        self.d_names, self.d_shapes, self.d_dtypes = req_names, req_shapes, req_dtypes\n",
    "    \n",
    "    def _parse_weight_shapes(self):\n",
    "        self.w_names, w_spec_dicts = zip(*self.args.weight_template.items())\n",
    "        default_init = self.args.default_init_method\n",
    "        self.w_shapes, self.w_inits = zip(*[(w_dict['shape'], \n",
    "                                             w_dict.get('init_method', default_init))\n",
    "                                             for w_dict in w_spec_dicts])\n",
    "        \n",
    "        sizes = [np.prod(wi) for wi in self.w_shapes]\n",
    "        if self.args.verbose:\n",
    "            print(\"Weight shapes are: {}, n_total_params: {}\".format(dict(zip(self.w_names, self.w_shapes)), \n",
    "                                                                     sum(sizes)))\n",
    "        \n",
    "    def _parse_optimization_method(self):\n",
    "        opt_dict = self.args.optimization_method.copy() # dict here\n",
    "        name = opt_dict['name']\n",
    "        opt_dict.pop('name')\n",
    "        params = opt_dict\n",
    "        self.optimization_method = self._optimization_method_builder(name, params)\n",
    "        \n",
    "    def _test_data_shape(self, data_nt):\n",
    "        data_pieces = [getattr(data_nt, name) for name in self.d_names]\n",
    "        true_shapes = [data_i.shape for data_i in data_pieces]\n",
    "        true_dtypes = [data_i.dtype for data_i in data_pieces]\n",
    "        \n",
    "        data_downcasted = []\n",
    "        for true_dt, req_dt, name_i, data_i in zip(true_dtypes, self.d_dtypes, self.d_names, data_pieces):\n",
    "            if true_dt == req_dt:\n",
    "                data_downcasted.append(data_i)\n",
    "            else: # types does not fit\n",
    "                if self.args.allow_downcast:\n",
    "                    if str(req_dt)[:3] == str(req_dt)[:3]: # int8 ~= int32, float32 ~= float64\n",
    "                        if self.args.allow_downcast != 'silent':\n",
    "                            print(\"Trying to downcast %s %s->%s, you'd better fix it!\" % (name_i, true_dt, req_dt))\n",
    "                        data_downcasted.append(data_i.astype(req_dt))\n",
    "                        continue\n",
    "                raise AssertionError(\"Not all dtypes match, expected %s got %s\" % (self.d_dtypes, true_dtypes))\n",
    "                \n",
    "        const = assert_arr_shape(dict(zip(true_shapes, self.d_shapes)))\n",
    "        return const, record('Data', dict(zip(self.d_names, data_downcasted)))\n",
    "    \n",
    "    def _collect_exported_functions(self):\n",
    "        self._exported_functions = []\n",
    "        for key, val in self.__class__.__dict__.items():\n",
    "            if hasattr(val, 'compilation_required') and getattr(val, 'compilation_required'):\n",
    "                self._exported_functions.append((key,val))\n",
    "                \n",
    "        self._exported_functions.append(('loss', self.__class__.loss)) # we always need to compile loss\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def loss(self, theta, data, const):\n",
    "        pass\n",
    "            \n",
    "    @abc.abstractmethod\n",
    "    def step_callback(self, loss, theta, data, const, info):\n",
    "        # must return False when finished optimizing\n",
    "        pass\n",
    "    \n",
    "    # implemented in backend Loss\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, data, n_max_steps=100):\n",
    "        pass\n",
    "    \n",
    "    # inplemented in backend Loss\n",
    "    @abc.abstractmethod\n",
    "    def _optimization_method_builder(self, name, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    # inplemented in backend Loss\n",
    "    @abc.abstractmethod\n",
    "    def _compile_and_disable_exported_functions(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a core.py\n",
    "\n",
    "class AutogradModelLoss(AbstractModelLoss):\n",
    "    import autograd\n",
    "    import autograd.numpy as anp\n",
    "    import climin\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._compile_and_disable_exported_functions()\n",
    "        self._build()\n",
    "    \n",
    "    def fit(self, data, n_max_steps=100):\n",
    "        x, theta, loss1d = self.__build_packed_loss()\n",
    "        loss1d_grad = self.autograd.grad(loss1d)\n",
    "        self.__test_run(loss1d, loss1d_grad, x, data)\n",
    "        data_iterator = self.__warp_data_into_iterator(data)\n",
    "        \n",
    "        opt = self.optimization_method(x, loss1d_grad, data_iterator)\n",
    "        with np.errstate(all='raise'): # raise errors on number overflows\n",
    "            for info in opt:\n",
    "                data_i = info['kwargs']['data']\n",
    "                const_i, data_i = self._test_data_shape(data_i)\n",
    "                our_info_i = {'n_iter': info['n_iter']}\n",
    "                loss_val_i = self.compiled.loss(theta, data_i, const_i)\n",
    "                ret = self.step_callback(loss_val_i, theta, data_i, const_i, our_info_i)\n",
    "                for behaviour in self.args.behaviours.values():\n",
    "                    behaviour.each_iteration(loss_val_i, theta, data_i, const_i, our_info_i)\n",
    "                if ret == False or info['n_iter'] == n_max_steps:\n",
    "                    break\n",
    "                    \n",
    "        self.behaviours = record('Behaviours', self.args.behaviours)\n",
    "        return theta\n",
    "    \n",
    "    def _build(self):\n",
    "        pass\n",
    "    \n",
    "    def _optimization_method_builder(self, name, opt_params):\n",
    "        clip_val = opt_params.pop('clip', None)\n",
    "        def clipped(grad):\n",
    "            if clip_val == None:\n",
    "                return grad\n",
    "            \n",
    "            def func(*argv, **kwargs):\n",
    "                g_val = grad(*argv, **kwargs)\n",
    "                g_clipped_val = self.anp.clip(g_val, -clip_val, clip_val)\n",
    "                return g_clipped_val\n",
    "            return func\n",
    "        \n",
    "        # external API name: internal name\n",
    "        kw_universal_mapping = {'learning_rate': 'step_rate', 'clip': '_'}\n",
    "        our_kwargs = {kw_universal_mapping[name]:value for name, value in opt_params.items()}\n",
    "        methods = {\n",
    "            'adam': lambda x, g, dat_it: self.climin.adam.Adam(x, clipped(g), args=dat_it, **our_kwargs)\n",
    "        }\n",
    "        \n",
    "        return methods[name]\n",
    "    \n",
    "    def __warp_data_into_iterator(self, data):\n",
    "        needs_warp = isinstance(data, dict) or not isinstance(data, collections.abc.Iterable)\n",
    "        data_seq = [data] if needs_warp else data\n",
    "        return itertools.cycle([([], {'data': to_record(data_i)}) for data_i in data_seq])\n",
    "    \n",
    "    def __build_packed_loss(self):\n",
    "        x_init, theta_init = self.__build_initial_x_theta()\n",
    "        def packed_loss(x, data):\n",
    "            const, data = self._test_data_shape(data)\n",
    "            theta = self.__unpack_x(x)\n",
    "            return self.raw.loss(theta, data, const) # need to compile loss before this line\n",
    "        return x_init, theta_init, packed_loss\n",
    "    \n",
    "    def __build_initial_x_theta(self):\n",
    "        x_init = self.anp.concatenate([self.anp.ravel(init_method(*shape)) \n",
    "                                 for shape, init_method \n",
    "                                 in zip(self.w_shapes, self.w_inits)])\n",
    "        sizes = [self.anp.prod(wi) for wi in self.w_shapes]\n",
    "        idx = self.anp.cumsum(sizes)\n",
    "        d = [(name, sub_x.reshape(shp)) for sub_x, name, shp \n",
    "             in zip(self.anp.split(x_init, idx), self.w_names, self.w_shapes)]\n",
    "        theta_init = record('ModelParameters', dict(d))\n",
    "        return x_init, theta_init\n",
    "    \n",
    "    def __unpack_x(self, x):\n",
    "        sizes = [self.anp.prod(wi) for wi in self.w_shapes]\n",
    "        idx = self.anp.cumsum(sizes)\n",
    "        d = [(name, sub_x.reshape(shp)) for sub_x, name, shp \n",
    "             in zip(self.anp.split(x, idx), self.w_names, self.w_shapes)]\n",
    "        theta = record('ModelParameters', dict(d))\n",
    "        return theta\n",
    "    \n",
    "    def __test_run(self, loss1d, loss1d_grad, x, data):\n",
    "        tmp_data_interator = self.__warp_data_into_iterator(data)\n",
    "        data_clim_pack = next(tmp_data_interator)\n",
    "        if self.args.test_forward_run:\n",
    "            try:\n",
    "                loss1d(x, **data_clim_pack[1])\n",
    "            except:\n",
    "                print(\"Error while testing forward pass!\")\n",
    "                raise\n",
    "\n",
    "            try:\n",
    "                loss1d_grad(x, **data_clim_pack[1])\n",
    "            except:\n",
    "                print(\"Error while testing backward pass!\")\n",
    "                raise\n",
    "                \n",
    "    def _compile_and_disable_exported_functions(self):\n",
    "        import copy\n",
    "        self.compiled = SimpleNamespace()\n",
    "        self.raw = SimpleNamespace()\n",
    "        \n",
    "        def error_stub(for_name):\n",
    "            def stub(*argv, **kwargs):\n",
    "                raise RuntimeError(\"You shouldn't use model.{0} in user code\"\n",
    "                                   \", use model.compiled.{0} for execution (in step_callback);\"\n",
    "                                   \" and self.raw.{0} for symbolic function (in loss\\other\"\n",
    "                                   \" compiled funcs)\".format(for_name))\n",
    "            return stub\n",
    "        \n",
    "        def self_func_stub(func):\n",
    "            # stub lives in namespace and does not get self\n",
    "            # but the original method self.func needs self, \n",
    "            # but it has it binded already \n",
    "            def stub(*argv, **kwargs):\n",
    "                return func(self, *argv, **kwargs)\n",
    "            return stub\n",
    "        \n",
    "        ## in case of autograd we don't do anything actually\n",
    "        ## we just forward same calls into self.compiled.func_name \n",
    "        for func_name, func in self._exported_functions:\n",
    "            func_copy = copy.copy(func)\n",
    "            self_stub = self_func_stub(func_copy)\n",
    "            setattr(self.compiled, func_name, self_stub)\n",
    "            setattr(self.raw, func_name, self_stub)\n",
    "            func_stub = error_stub(func_name)\n",
    "            setattr(self.__class__, func_name, func_stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a core.py\n",
    "\n",
    "class TheanoModelLoss(AbstractModelLoss):\n",
    "    import theano\n",
    "    import theano.tensor as T\n",
    "    import lasagne\n",
    "    \n",
    "    def __init__(self, **kwargs):        \n",
    "        super().__init__(**kwargs)\n",
    "        self.__build_theta_data_symbols()\n",
    "        self._compile_and_disable_exported_functions()\n",
    "        self._build()\n",
    "    \n",
    "    def fit(self, data, n_max_steps=100):        \n",
    "        ## theano => no need to pass shared vars there, there are in the context\n",
    "        self.__test_run(self.compiled.current_loss, self.compiled.grad, data)\n",
    "        data_iterator = self.__warp_data_into_iterator(data)\n",
    "\n",
    "        for i in range(n_max_steps+1): # to make it same as in autograd == n_max_steps\n",
    "            data_batch = next(data_iterator)\n",
    "            data_batch_record = to_record(data_batch)\n",
    "            const_i, data_batch_record = self._test_data_shape(data_batch_record)            \n",
    "            loss_val = self.compiled.perform_grad_step(**data_batch)\n",
    "            theta_vals_record = record('WeightValues', \n",
    "                                       {name:shared_var.get_value(borrow=True)\n",
    "                                        for name, shared_var in self.symbols['theta_shared_odict'].items()})\n",
    "            info = {'n_iter': i}\n",
    "            ret = self.step_callback(loss_val, theta_vals_record, data_batch_record, const_i, info)\n",
    "            for behaviour in self.args.behaviours.values():\n",
    "                behaviour.each_iteration(loss_val, theta_vals_record, data_batch_record, const_i, info)\n",
    "            if ret == False:\n",
    "                break\n",
    "        \n",
    "        self.behaviours = record('Behaviours', self.args.behaviours)\n",
    "        return theta_vals_record\n",
    "    \n",
    "    def _build(self):\n",
    "        if self.args.verbose:\n",
    "            print(self.theano.config.mode)\n",
    "            print('Building learning step function and gradient ..', end='')\n",
    "\n",
    "            \n",
    "        loss_expr = self.raw.loss(self.symbols['theta_nt'], self.symbols['data_nt'], \n",
    "                                  self.symbols['const_nt'])\n",
    "        loss_grad_expr = self.T.grad(loss_expr, self.symbols['param_vars'])        \n",
    "        updates = self.optimization_method(loss_grad_expr, self.symbols['param_vars'])\n",
    "        self.compiled.perform_grad_step = self.theano.function(self.symbols['input_vars'], \n",
    "                                                               loss_expr, updates=updates)\n",
    "        self.compiled.current_loss = self.theano.function(self.symbols['input_vars'], loss_expr)\n",
    "        self.compiled.grad = self.theano.function(self.symbols['input_vars'], loss_grad_expr)\n",
    "        \n",
    "        if self.args.verbose:\n",
    "            print('.. done')\n",
    "            \n",
    "        for func_name, func in self.raw.__dict__.items():\n",
    "            if self.args.verbose:\n",
    "                print('Building ', func_name, end='')\n",
    "\n",
    "            # for export_data restrinctions\n",
    "            local_input_vars = ([self.symbols['data_tensor_odict'][name] for name in func.data_list]\n",
    "                                if func.data_list else self.symbols['input_vars'])\n",
    "            \n",
    "            local_full_input_vars = (list(self.symbols['theta_tensor_odict'].values()) \n",
    "                                     + local_input_vars)\n",
    "            func_expr = func(self.symbols['theta_as_data_nt'], \n",
    "                             self.symbols['data_nt'], \n",
    "                             self.symbols['const_nt'])\n",
    "            func_compiled = self.theano.function(local_full_input_vars, func_expr)\n",
    "            func_compiled_warped = self.__warp_theano_function(func_compiled, data_list=func.data_list)\n",
    "            setattr(self.compiled, func_name, func_compiled_warped)\n",
    "\n",
    "            if self.args.verbose:\n",
    "                print('.. done')\n",
    "\n",
    "    def _optimization_method_builder(self, name, params):\n",
    "        clip_val = params.pop('clip', None)\n",
    "        clipped = ((lambda grads: [self.T.clip(grad, -clip_val, clip_val) for grad in grads])\n",
    "                   if clip_val else (lambda grad: grad))\n",
    "        # external API name: self.internal name\n",
    "        kw_universal_mapping = {'learning_rate': 'learning_rate'}\n",
    "        our_kwargs = {kw_universal_mapping[name]:value for name, value in params.items()}\n",
    "        methods = {\n",
    "            'adam': lambda grads, params: self.lasagne.updates.adam(clipped(grads), params, **our_kwargs)\n",
    "        }\n",
    "        return methods[name]\n",
    "    \n",
    "    def __build_theta_data_symbols(self):\n",
    "        d = {}\n",
    "        d['theta_nt'], d['theta_shared_odict'] = self.__build_initial_theta_shared_dict()\n",
    "        d['data_nt'], d['data_tensor_odict'] = self.__build_data_tensor_dict()\n",
    "        d['theta_as_data_nt'], d['theta_tensor_odict'] = self.__build_theta_tensor_dict()\n",
    "        tensor_shapes = [x.shape for x in d['data_tensor_odict'].values()]\n",
    "        \n",
    "        ## actually just a view of data_nt, does not need to be passed to theano.function\n",
    "        d['const_nt'] = extract_const_vars_from_tensors(dict(zip(tensor_shapes, self.d_shapes)))\n",
    "        \n",
    "        d['input_vars'] = list(d['data_tensor_odict'].values())\n",
    "        d['param_vars'] = list(d['theta_shared_odict'].values())\n",
    "        \n",
    "        self.symbols = d\n",
    "    \n",
    "    def __build_data_tensor_dict(self):\n",
    "        dim_type_mapping = {0: self.T.scalar, 1: self.T.vector, 2:self.T.matrix, \n",
    "                            3:self.T.tensor3, 4:self.T.tensor4}\n",
    "        data_tensor_odict = collections.OrderedDict()\n",
    "        for name, shape, dtype in zip(self.d_names, self.d_shapes, self.d_dtypes):\n",
    "            dim_i = len(shape)\n",
    "            if dim_i == 1 and shape[0] == 1: # scalar\n",
    "                dim_i = 0\n",
    "            try:\n",
    "                tensor_type = dim_type_mapping[dim_i]\n",
    "            except KeyError:\n",
    "                raise ValueError(\"We do not support arrays of dimentions != [0..4], sorry :(\")\n",
    "            data_tensor_odict[name] = tensor_type(name, dtype=dtype)\n",
    "        data_nt = record('DataTensors', data_tensor_odict)\n",
    "        return data_nt, data_tensor_odict\n",
    "    \n",
    "    def __build_theta_tensor_dict(self):\n",
    "        ## same but for THETA AS DATA input for self.compiled.func(theta, ...) calls\n",
    "        dim_type_mapping = {0: self.T.scalar, 1: self.T.vector, 2:self.T.matrix, \n",
    "                            3:self.T.tensor3, 4:self.T.tensor4}\n",
    "        theta_tensor_odict = collections.OrderedDict()\n",
    "        for name, shape in zip(self.w_names, self.w_shapes):\n",
    "            dim_i = len(shape)                \n",
    "            try:\n",
    "                tensor_type = dim_type_mapping[dim_i]\n",
    "            except KeyError:\n",
    "                raise ValueError(\"We do not support arrays of dimentions != [0..4], sorry :(\")\n",
    "            theta_tensor_odict[name] = tensor_type(name)\n",
    "        theta_as_data_nt = record('ModelParametrTensors', theta_tensor_odict)\n",
    "        return theta_as_data_nt, theta_tensor_odict\n",
    "\n",
    "    def __build_initial_theta_shared_dict(self):\n",
    "        theta_shared_odict = collections.OrderedDict()\n",
    "        for name, shape, init_func in zip(self.w_names, self.w_shapes, self.w_inits):\n",
    "            init_value = init_func(*shape)\n",
    "            shared_var = self.theano.shared(init_value, name=name)\n",
    "            theta_shared_odict[name] = shared_var\n",
    "        \n",
    "        theta_nt = record('ModelParameterSharedVars', theta_shared_odict)\n",
    "        return theta_nt, theta_shared_odict\n",
    "    \n",
    "    def __warp_theano_function(self, th_func, data_list = None):\n",
    "        def func(theta, data, const):\n",
    "            input_dict = {}\n",
    "            input_dict.update({w_name:getattr(theta, w_name).astype(self.theano.config.floatX) \n",
    "                               for w_name in self.w_names})\n",
    "            input_dict.update({d_name:getattr(data, d_name) for d_name in (data_list or self.d_names)})\n",
    "            ret_val = th_func(**input_dict)\n",
    "            return ret_val\n",
    "        return func\n",
    "    \n",
    "    def __warp_data_into_iterator(self, data):\n",
    "        needs_warp = isinstance(data, dict) or not isinstance(data, collections.abc.Iterable)\n",
    "        data_seq = [data] if needs_warp else data\n",
    "        return itertools.cycle(data_seq)\n",
    "    \n",
    "    def _compile_and_disable_exported_functions(self):\n",
    "        self.compiled = SimpleNamespace()\n",
    "        self.raw = SimpleNamespace()\n",
    "        \n",
    "        def method_stub(for_name):\n",
    "            def stub(*argv, **kwargs):\n",
    "                raise RuntimeError(\"You shouldn't call model.{0} in user code\"\n",
    "                                   \", use model.compiled.{0}; original function \"\n",
    "                                   \"is still avaliable at self.raw.{0}\".format(for_name))\n",
    "            return stub\n",
    "        \n",
    "        def new_func_stub(func):\n",
    "            # funcs are just function (unbonded) from class\n",
    "            # stub lives in namespace and does not get self\n",
    "            # but the original method __class__.func needs self\n",
    "            def stub(*argv, **kwargs):\n",
    "                return func(self, *argv, **kwargs)\n",
    "            stub.data_list = getattr(func, 'data_list', None)\n",
    "            return stub\n",
    "        \n",
    "        ## in case of autograd we don't do anything actually\n",
    "        ## we just forward same calls into self.compiled.func_name \n",
    "        for func_name, func in self._exported_functions:\n",
    "            setattr(self.__class__, func_name, method_stub(func_name))\n",
    "            setattr(self.raw, func_name, new_func_stub(func))\n",
    "    \n",
    "    def __test_run(self, loss_func, grad_func, data):\n",
    "        data_pack = next(self.__warp_data_into_iterator(data))\n",
    "        if self.args.test_forward_run:\n",
    "            try:\n",
    "                loss_func(**data_pack)\n",
    "            except:\n",
    "                print(\"Error while testing forward pass!\")\n",
    "                raise\n",
    "\n",
    "            try:\n",
    "                grad_func(**data_pack)\n",
    "            except:\n",
    "                print(\"Error while testing backward pass!\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a core.py\n",
    "\n",
    "class OpsBundle:\n",
    "    def __init__(self, bk):\n",
    "        self.bk = bk\n",
    "        \n",
    "    def softmax(self, A, axis=-1):\n",
    "        # theano does not support elipses; above same as [..., newaxis]\n",
    "        dim_new_axis =[slice(None, None), ]*(A.ndim)\n",
    "        dim_new_axis[axis] = self.bk.newaxis\n",
    "        expA = self.bk.exp(A - self.bk.max(A, axis=axis)[tuple(dim_new_axis)])\n",
    "        s = self.bk.sum(expA, axis=axis)\n",
    "        return expA / s[dim_new_axis]\n",
    "    \n",
    "class BaseBackend:\n",
    "    def __init__(self, lookup_object):\n",
    "        self.__lookup_object = lookup_object\n",
    "        self.ops = OpsBundle(self)\n",
    "        \n",
    "    @property\n",
    "    def bk(self):\n",
    "        return self.__lookup_object\n",
    "        \n",
    "    # decorator\n",
    "    @staticmethod\n",
    "    def export(func):\n",
    "        func.compilation_required = True\n",
    "        return func\n",
    "\n",
    "    @staticmethod\n",
    "    def export_data(data_list):\n",
    "        def tmp(func):\n",
    "            func.compilation_required = True\n",
    "            func.data_list = data_list\n",
    "            return func\n",
    "        return tmp\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.__dict__:\n",
    "            return self.__dict__[name]\n",
    "        else:\n",
    "            return getattr(self.__lookup_object, name)\n",
    "\n",
    "class AutoGradBackend(BaseBackend):\n",
    "    def __init__(self):\n",
    "        import autograd.numpy as __bk\n",
    "        super().__init__(__bk)\n",
    "        self.__dict__.update({\n",
    "            'ModelLoss': AutogradModelLoss,\n",
    "            'assert_arr_shape': assert_arr_shape\n",
    "        })\n",
    "        self.ModelLoss.bk = self # now in Loss we can access self.bk\n",
    "        \n",
    "    @staticmethod\n",
    "    def eq(a, b):\n",
    "        return a == b\n",
    "\n",
    "class TheanoBackend(BaseBackend):\n",
    "    import sys\n",
    "    import theano\n",
    "    import theano.tensor as T\n",
    "    def __init__(self, mode='FAST_COMPILE'):\n",
    "        self.sys.setrecursionlimit(100000)\n",
    "        self.theano.config.mode = mode\n",
    "        super().__init__(self.T)\n",
    "        \n",
    "        self.__dict__.update({\n",
    "            'ModelLoss': TheanoModelLoss,\n",
    "            'abs': abs,\n",
    "            'newaxis': None,\n",
    "            'assert_arr_shape': extract_const_vars_from_tensors,\n",
    "        })\n",
    "        \n",
    "        self.ModelLoss.bk = self\n",
    "        \n",
    "    @classmethod\n",
    "    def eq(cls, a, b):\n",
    "        return cls.T.eq(a, b)\n",
    "        \n",
    "    @classmethod\n",
    "    def sign(cls, a):\n",
    "        return cls.T.sgn(a)\n",
    "    \n",
    "    @classmethod\n",
    "    def multiply(cls, a, b):\n",
    "        return a*b\n",
    "    \n",
    "    @classmethod\n",
    "    def indices(cls, shape):\n",
    "        mgrid_input = [slice(0, sh) for sh in shape]\n",
    "        return cls.T.mgrid[mgrid_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a core.py\n",
    "\n",
    "class UnitBundle:\n",
    "    def __init__(self, bk, name, weight_template, data_template):\n",
    "        self.__name = name\n",
    "        self.__prefix = name+'__'\n",
    "        self.__template = weight_template\n",
    "        self.global_template = {self.__prefix+key:val for key, val in weight_template.items()}\n",
    "        self.data_template = data_template\n",
    "        self.global_keys = set(self.global_template.keys())\n",
    "        self.registered = False\n",
    "        self.bk = bk\n",
    "    \n",
    "    def apply(self, global_theta, data_dict, **kwargs):\n",
    "        if not self.registered:\n",
    "            raise RuntimeError(\"The %s bundle was not registered via weight_bundles\" % self.__name)\n",
    "        local_theta = {\n",
    "            key[len(self.__prefix):]:val # adapt - shortened key\n",
    "            for key, val in global_theta._asdict().items()\n",
    "            if key in self.global_keys\n",
    "        }\n",
    "        local_theta_nt = record('LocalBundleWeights', local_theta)\n",
    "        \n",
    "        d_names = self.data_template.keys()\n",
    "        if set(d_names) != set(data_dict.keys()):\n",
    "            raise ValueError(\"Exported %s data keys, got %s\" \n",
    "                             % (set(d_names), set(data_dict.keys())))\n",
    "        true_shapes = [data_dict[name].shape for name in d_names]\n",
    "        required_shapes = [self.data_template[name] for name in d_names]\n",
    "        local_const = self.bk.assert_arr_shape(dict(zip(true_shapes, required_shapes)))\n",
    "        local_data_nt = record('LocalBundleData', data_dict)\n",
    "        return self._func(local_theta_nt, local_data_nt, local_const, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def weight_template(self):\n",
    "        return self.global_template\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _func(self): # may have whatever interface!\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
